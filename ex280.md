- [Objectives](#objectives)
  - [Manage OpenShift Container Platform](#manage-openshift-container-platform)
  - [Manage users and policies](#manage-users-and-policies)
  - [Control access to resources](#control-access-to-resources)
  - [Configure networking components](#configure-networking-components)
  - [Configure pod scheduling](#configure-pod-scheduling)
  - [Configure cluster scaling](#configure-cluster-scaling)
- [Solutions/Commands](#solutionscommands)
  - [Manage OpenShift Container Platform](#manage-openshift-container-platform-1)
    - [Create project](#create-project)
  - [Control access to resources](#control-access-to-resources-1)
    - [HTPasswd](#htpasswd)
    - [Modify users passwd](#modify-users-passwd)
    - [Remove users / kubeadmin](#remove-users--kubeadmin)
    - [Roles](#roles)
  - [Secrets](#secrets)
    - [Create secrets](#create-secrets)
    - [Service accounts](#service-accounts)
  - [Configure networking components](#configure-networking-components-1)
    - [Troubleshoot software defined networking](#troubleshoot-software-defined-networking)
    - [Create and edit external routes](#create-and-edit-external-routes)
    - [Control cluster network ingress](#control-cluster-network-ingress)
    - [Create a self signed certificate](#create-a-self-signed-certificate)
    - [Secure routes using TLS certificates](#secure-routes-using-tls-certificates)
  - [Configure pod scheduling](#configure-pod-scheduling-1)
    - [Limit resource usage](#limit-resource-usage)
    - [Scale applications to meet increased demand](#scale-applications-to-meet-increased-demand)
    - [Control pod placement across cluster nodes](#control-pod-placement-across-cluster-nodes)
  - [Configure cluster scaling](#configure-cluster-scaling-1)
    - [Manually control the number of cluster workers](#manually-control-the-number-of-cluster-workers)
    - [Automatically scale the number of cluster workers](#automatically-scale-the-number-of-cluster-workers)
# Objectives
## Manage OpenShift Container Platform
- Use the command-line interface to manage and configure an OpenShift cluster
- Use the web console to manage and configure an OpenShift cluster
- Create and delete projects
- Import, export, and configure Kubernetes resources
- Examine resources and cluster status
- View logs
- Monitor cluster events and alerts
- Troubleshoot common cluster events and alerts
- Use product documentation
## Manage users and policies
- Configure the HTPasswd identity provider for authentication
- Create and delete users
- Modify user passwords
- Modify user and group permissions
- Create and manage groups
## Control access to resources
- Define role-based access controls
- Apply permissions to users
- Create and apply secrets to manage sensitive information
- Create service accounts and apply permissions using security context constraints
## Configure networking components
- Troubleshoot software defined networking
- Create and edit external routes
- Control cluster network ingress
- Create a self signed certificate
- Secure routes using TLS certificates
## Configure pod scheduling
- Limit resource usage
- Scale applications to meet increased demand
- Control pod placement across cluster nodes
## Configure cluster scaling
- Manually control the number of cluster workers
- Automatically scale the number of cluster workers

# Solutions/Commands

## Manage OpenShift Container Platform

### Create project

```
oc new-project roman-p2
oc new-app rails-postgresql-example

oc logs postgresql-deploy

oc get all

oc get -o yaml res res.yml


oc projects

oc delete project XYZ
```

## Control access to resources

### HTPasswd
Create file:
```
htpasswd -c -B -b htpasswd.file roman roman

```
Create secret:
```
oc create secret general htpass-secret --from-file=htpasswd=htpasswd.file -n openshift-config
```

Custom resource (cr.yml):
```
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: my_htpasswd_provider 
    mappingMethod: claim 
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpass-secret 
```

```
oc apply -f cr.yml
```

```
oc adm policy add-role-to-user admin roman
oc adm policy add-role-to-user edit roman
oc adm policy add-role-to-user view roman
oc describe rolebinding.rbac -n roman-p1
oc adm policy add-cluster-role-to-user cluster-admin roman
```

```
oc adm groups new roman-g1
oc adm groups add-users roman-g1 roman
oc adm policy add-role-to-group admin roman-g1
```

### Modify users passwd
oc extract secret/htpass-secret --to - -n openshift-config > users.htpasswd
htpasswd -B users.htpasswd roman roman2
oc create secret generic htpass-psecret --from-file=htpasswd=users.htpasswd --dry-run -o -n openshift-config | or replace -f -


### Remove users / kubeadmin

```
oc delete secrets kubeadmin -n kube-system
```

### Roles

```
oc create role podview --verb=get --resource=pod -n blue
oc create clusterrole podviewonly --verb=get --resource=pod 
```

## Secrets

### Create secrets

Create secret and import:
vim secret.yml
```
apiVersion: v1
kind: Secret
metadata:
  name: test-secret
  namespace: roman-p1
type: Opaque 
data: 
  username: dmFsdWUtMQ0K 
  password: dmFsdWUtMg0KDQo=
stringData: 
  hostname: myapp.mydomain.com 
```
oc create -f secret.yml

### Service accounts

```
oc get sa
oc create sa service-ac1
oc describe sa/service-ac1

oc policy add-role-to-user view system:serviceaccount:roman-p1:service-ac1
```

sec-cont.yml
```
kind: SecurityContextConstraints
apiVersion: security.openshift.io/v1
metadata:
  name: scc-admin
allowPrivilegedContainer: true
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
fsGroup:
  type: RunAsAny
supplementalGroups:
  type: RunAsAny
users:
- service-ac1
requiredDropCapabilities:
- KILL
- MKNOD
- SYS_CHROOT
```

## Configure networking components
### Troubleshoot software defined networking
NETWORK

```
oc get -n openshift-network-operator deployment/network-operator

oc get clusteroperator/network

oc describe network.config/cluster

oc logs -n openshift-network-operator deployment/network-operator --tail 10
```

DNS

```
oc get -n openshift-dns-operator deployment/dns-operator

oc get clusteroperator/dns

oc describe clusteroperator/dns

oc logs -n openshift-dns-operator deployment/dns-operator
```

ROUTES

```
oc get endpoints -n roman-p2

oc get pods -n roman-p2 --template='{{range.items}}HostIP:{{status.hostIP}}'

oc get route -n roman-p2

```
### Create and edit external routes

CREATE

```
oc expose service xxxxx
or
oc expose service sssss -l name=label name --name=routename
or
oc expose service sssss --port=port --protocol="protocal"
or
oc expose service sssss --path=path
```

EDIT

```
// add a timeout
oc annotate route route-r1 --overwrite haproxy.router.openshift
or
// add a desired cookie name
oc annotate route route-r1
or
// restrict access by whitelisting an IP
oc annotate route route-r1
or
// enable rate limit
oc annotate route route-r1


```

### Control cluster network ingress

Ingress Resource
```
ingress_res.yml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress
EOF

oc create -f ingress_res.yml
```

### Create a self signed certificate
```
openssl req -x509 -newkey rsa:4096 -nodes -keyout cert.key -out cert.crt
openssl rsa -in paass_file.key -out key_file.key
```
### Secure routes using TLS certificates
```
oc create route edge --service=frontend --cert=tls.crt --key=tls.key --hostname=www.example.com
```

```
apiVersion: v1
kind: Route
metadata:
  name: frontend
spec:
  host: www.example.com
  to:
    kind: Service
    name: frontend
  tls:
    termination: reencrypt
    key: |-
      -----BEGIN PRIVATE KEY-----
      [...]
      -----END PRIVATE KEY-----
    certificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----  
```      

## Configure pod scheduling
### Limit resource usage
core-object-counts.yml
```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: core-object-counts
spec:
  hard:
    configmaps: "10" 
    persistentvolumeclaims: "4" 
    replicationcontrollers: "20" 
    secrets: "10" 
    services: "10" 
    services.loadbalancers: "2" 
```
oc create -f core-object-counts.yml

oc create quota qname --hard=count/res.group=q1, .....
###  Scale applications to meet increased demand

oc scale --replicas=3 dc/deployment-r1

oc autoscale dc/<dc-name> \
  --min <number> \
  --max <number> \
  --cpu-percent=<percent> 

###  Control pod placement across cluster nodes

## Configure cluster scaling
###  Manually control the number of cluster workers

###  Automatically scale the number of cluster workers