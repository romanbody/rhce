- [Objectives](#objectives)
  - [Manage OpenShift Container Platform](#manage-openshift-container-platform)
  - [Manage users and policies](#manage-users-and-policies)
  - [Control access to resources](#control-access-to-resources)
  - [Configure networking components](#configure-networking-components)
  - [Configure pod scheduling](#configure-pod-scheduling)
  - [Configure cluster scaling](#configure-cluster-scaling)
- [Solutions/Commands](#solutionscommands)
  - [Manage OpenShift Container Platform](#manage-openshift-container-platform-1)
    - [Create project](#create-project)
  - [Control access to resources](#control-access-to-resources-1)
    - [HTPasswd](#htpasswd)
    - [Modify users passwd](#modify-users-passwd)
    - [Remove users / kubeadmin](#remove-users--kubeadmin)
    - [Roles](#roles)
  - [Secrets](#secrets)
    - [Create secrets](#create-secrets)
    - [SSC & Service accounts](#ssc--service-accounts)
      - [Create SA](#create-sa)
      - [Get SCC](#get-scc)
      - [Give SCC](#give-scc)
      - [Set SA to Deploy](#set-sa-to-deploy)
  - [Configure networking components](#configure-networking-components-1)
    - [Troubleshoot software defined networking](#troubleshoot-software-defined-networking)
    - [Create and edit external routes](#create-and-edit-external-routes)
    - [Control cluster network ingress](#control-cluster-network-ingress)
    - [Create a self signed certificate](#create-a-self-signed-certificate)
    - [Secure routes using TLS certificates](#secure-routes-using-tls-certificates)
      - [Create Passthrough](#create-passthrough)
        - [Create TLS Secret](#create-tls-secret)
        - [Mount TLS TO Deploy](#mount-tls-to-deploy)
        - [Double Check TLS SAN Info](#double-check-tls-san-info)
        - [Create Passthrough](#create-passthrough-1)
      - [Create a private key, CSR and certificate.](#create-a-private-key-csr-and-certificate)
  - [Configure pod scheduling](#configure-pod-scheduling-1)
    - [Limit resource usage](#limit-resource-usage)
    - [Scale applications to meet increased demand](#scale-applications-to-meet-increased-demand)
    - [Control pod placement across cluster nodes](#control-pod-placement-across-cluster-nodes)
      - [Option 1 - taints & tolerations](#option-1---taints--tolerations)
      - [Option 2 - labels](#option-2---labels)
  - [Configure cluster scaling](#configure-cluster-scaling-1)
    - [Manually control the number of cluster workers](#manually-control-the-number-of-cluster-workers)
    - [Automatically scale the number of cluster workers](#automatically-scale-the-number-of-cluster-workers)
# Objectives
## Manage OpenShift Container Platform
- Use the command-line interface to manage and configure an OpenShift cluster
- Use the web console to manage and configure an OpenShift cluster
- Create and delete projects
- Import, export, and configure Kubernetes resources
- Examine resources and cluster status
- View logs
- Monitor cluster events and alerts
- Troubleshoot common cluster events and alerts
- Use product documentation
## Manage users and policies
- Configure the HTPasswd identity provider for authentication
- Create and delete users
- Modify user passwords
- Modify user and group permissions
- Create and manage groups
## Control access to resources
- Define role-based access controls
- Apply permissions to users
- Create and apply secrets to manage sensitive information
- Create service accounts and apply permissions using security context constraints
## Configure networking components
- Troubleshoot software defined networking
- Create and edit external routes
- Control cluster network ingress
- Create a self signed certificate
- Secure routes using TLS certificates
## Configure pod scheduling
- Limit resource usage
- Scale applications to meet increased demand
- Control pod placement across cluster nodes
## Configure cluster scaling
- Manually control the number of cluster workers
- Automatically scale the number of cluster workers

# Solutions/Commands

## Manage OpenShift Container Platform

### Create project

```
oc new-project roman-p2
oc new-app rails-postgresql-example

oc logs postgresql-deploy

oc get all

oc get -o yaml res res.yml


oc projects

oc delete project XYZ
```

## Control access to resources

### HTPasswd
Create file:
```
htpasswd -c -B -b htpasswd.file roman roman

```
Create secret:
```
oc create secret general htpass-secret --from-file=htpasswd=htpasswd.file -n openshift-config
```

Custom resource (cr.yml):
```
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: my_htpasswd_provider 
    mappingMethod: claim 
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpass-secret 
```

```
oc apply -f cr.yml
```

```
oc adm policy add-role-to-user admin roman
oc adm policy add-role-to-user edit roman
oc adm policy add-role-to-user view roman
oc describe rolebinding.rbac -n roman-p1
oc adm policy add-cluster-role-to-user cluster-admin roman
```

```
oc adm groups new roman-g1
oc adm groups add-users roman-g1 roman
oc adm policy add-role-to-group admin roman-g1
```

### Modify users passwd
oc extract secret/htpass-secret --to - -n openshift-config > users.htpasswd
htpasswd -B users.htpasswd roman roman2
oc create secret generic htpass-psecret --from-file=htpasswd=users.htpasswd --dry-run -o -n openshift-config | or replace -f -


### Remove users / kubeadmin

```
oc delete secrets kubeadmin -n kube-system
```

### Roles

```
oc create role podview --verb=get --resource=pod -n blue
oc create clusterrole podviewonly --verb=get --resource=pod 
```

Remove the self-provisioner cluster role from the
system:authenticated:oauth virtual group, which deletes the selfprovisioners
role binding. You can safely ignore the warning about your changes
being lost.

```
oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated:oauth
oc describe clusterrolebindings self-provisioners
```
## Secrets

### Create secrets

Create secret and import:
vim secret.yml
```
apiVersion: v1
kind: Secret
metadata:
  name: test-secret
  namespace: roman-p1
type: Opaque 
data: 
  username: dmFsdWUtMQ0K 
  password: dmFsdWUtMg0KDQo=
stringData: 
  hostname: myapp.mydomain.com 
```
oc create -f secret.yml


oc create secret generic romansec1 --from-literal key1=secret1  --from-literal key2=secret2
oc create secret tls secret-tls --cert /path-to-certificate --key /path-to-key
oc create secret generic demo-secret --from-literal user=demo-user --from-literal root_password=zT1KTgk

oc set data secret/romansec1 --from-literal key1=secret1
### SSC & Service accounts


Deploy new app, check logs, identify which SSC will solve a issue and apply fix:

```
oc new-app --name gitlab  --docker-image quay.io/redhattraining/gitlab-ce:8.4.3-ce.0
oc get pods
oc logs pod/gitlab-7d67db7875-gcsjl
oc get pod/gitlab-7d67db7875-gcsjl -o yaml | oc adm policy scc-subject-review -f -
oc create sa gitlab-sa
oc adm policy add-scc-to-user anyuid -z gitlab-sa
oc set serviceaccount deployment/gitlab gitlab-sa
oc get pods
```


```
oc get sa
oc create sa service-ac1
oc describe sa/service-ac1

oc policy add-role-to-user view system:serviceaccount:roman-p1:service-ac1
```

sec-cont.yml
```
kind: SecurityContextConstraints
apiVersion: security.openshift.io/v1
metadata:
  name: scc-admin
allowPrivilegedContainer: true
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: RunAsAny
fsGroup:
  type: RunAsAny
supplementalGroups:
  type: RunAsAny
users:
- service-ac1
requiredDropCapabilities:
- KILL
- MKNOD
- SYS_CHROOT
```


  #### Create SA
  oc create serviceaccount [SA NAME]
  #### Get SCC
  oc get scc
  #### Give SCC
  oc adm policy add-scc-to-user anyuid -z [SANAME]
  #### Set SA to Deploy
  oc set serviceaccount deployment/[deployname] [SA Name]

## Configure networking components
### Troubleshoot software defined networking
NETWORK

```
oc get -n openshift-network-operator deployment/network-operator

oc get clusteroperator/network

oc describe network.config/cluster

oc logs -n openshift-network-operator deployment/network-operator --tail 10
```

DNS

```
oc get -n openshift-dns-operator deployment/dns-operator

oc get clusteroperator/dns

oc describe clusteroperator/dns

oc logs -n openshift-dns-operator deployment/dns-operator
```

ROUTES

```
oc get endpoints -n roman-p2

oc get pods -n roman-p2 --template='{{range.items}}HostIP:{{status.hostIP}}'

oc get route -n roman-p2

```
### Create and edit external routes

CREATE

```
oc expose service xxxxx
or
oc expose service sssss -l name=label name --name=routename
or
oc expose service sssss --port=port --protocol="protocal"
or
oc expose service sssss --path=path
```

EDIT

```
// add a timeout
oc annotate route route-r1 --overwrite haproxy.router.openshift
or
// add a desired cookie name
oc annotate route route-r1
or
// restrict access by whitelisting an IP
oc annotate route route-r1
or
// enable rate limit
oc annotate route route-r1


```

### Control cluster network ingress

Ingress Resource
```
ingress_res.yml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress
EOF

oc create -f ingress_res.yml
```

### Create a self signed certificate
```
openssl req -x509 -newkey rsa:4096 -nodes -keyout cert.key -out cert.crt
openssl rsa -in paass_file.key -out key_file.key
```
### Secure routes using TLS certificates
```
oc create route edge --service=frontend --cert=tls.crt --key=tls.key --hostname=www.example.com
```

```
apiVersion: v1
kind: Route
metadata:
  name: frontend
spec:
  host: www.example.com
  to:
    kind: Service
    name: frontend
  tls:
    termination: reencrypt
    key: |-
      -----BEGIN PRIVATE KEY-----
      [...]
      -----END PRIVATE KEY-----
    certificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----  
```      

oc create route edge  --service api-frontend --hostname api.apps.acme.com  --key api.key --cert api.crt

#### Create Passthrough
  ##### Create TLS Secret
  oc create secret tls [SEC Name] --cert [CRT NAME] --key [KEY name]
  
  ##### Mount TLS TO Deploy
  oc set volume deployment/[DEPOY_NAME] --add --tpye secret --secret-name [SEC Name] --mount-path [TLS Path]

  ##### Double Check TLS SAN Info
  openssl -in [CRT file.cert] -noout -ext subjectAltName

  ##### Create Passthrough
  oc create route passthrough --service [SVC Name] --hostname Domain.name.com

#### Create a private key, CSR and certificate.

```
openssl genrsa -out php.key 2048
openssl req -new -key php.key -out php.csr   -subj "/C=GB/ST=London/L=London/O=IT/OU=IT/CN=www.example.com"
openssl x509 -req -days 366 -in php.csr        -signkey php.key -out php.crt
```

Generate a route using the above certificate and key:

```
oc get svc
oc create route edge --service=my-php-service \
    --hostname=www.example.com \
    --key=php.key --cert=php.crt \
    --insecure-policy=Redirect
```

## Configure pod scheduling
### Limit resource usage
core-object-counts.yml
```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: core-object-counts
spec:
  hard:
    configmaps: "10" 
    persistentvolumeclaims: "4" 
    replicationcontrollers: "20" 
    secrets: "10" 
    services: "10" 
    services.loadbalancers: "2" 
```
oc create -f core-object-counts.yml

oc create quota qname --hard=count/res.group=q1, .....
###  Scale applications to meet increased demand

oc scale --replicas=3 dc/deployment-r1

oc autoscale dc/<dc-name> \
  --min <number> \
  --max <number> \
  --cpu-percent=<percent> 

###  Control pod placement across cluster nodes

oc adm manage-node --schedulable=true node1.lab.example.com

#### Option 1 - taints & tolerations

```
oc adm taint nodes node1 key1=value1:NoSchedule
oc adm taint nodes node1 key1=value1:NoExecute
oc adm taint nodes node1 key2=value2:NoSchedule
```

```
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
```
#### Option 2 - labels

```
oc label node node1 region=apps --overwrite
```

dc.yaml
```
nodeSelector:
  region: apps
```

## Configure cluster scaling
###  Manually control the number of cluster workers

###  Automatically scale the number of cluster workers